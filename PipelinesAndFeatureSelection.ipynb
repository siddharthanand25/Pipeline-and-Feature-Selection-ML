{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and Feature Selection\n",
    "###  Pipelines Objective\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "To use a Pipeline to repeat evaluation of the previous assignment done using Hold-Out testing. In the Algorithm Bias Assessment the results were coming out to be unstable with different train-test split, that's why this time we will be utilizing Cross-Validation technique with the Pipeline strategy to point out the diffeneces in both the strategies.\n",
    "</span>\n",
    "\n",
    "\n",
    "- To start with, load the hold-out evaluation strategy from the first assignment\n",
    "- Evaluate this strategy with the cross-Validation using Pipeline\n",
    "- Point out the results obtained by above two techniques.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "surv = pd.read_csv('survival.csv')   #loading the survival dataset as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetcount=surv['Class'].value_counts()\n",
    "print('total count of class type 1 in the survival dataset:',targetcount[1])\n",
    "print('total count of class type 2 in the survival dataset:',targetcount[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = surv.drop('Class', axis=1)   #X will become the independent variable for the models.\n",
    "y = surv['Class']                #y will become the dependent vaiables.\n",
    "X.shape, y.shape                 # checking rows and columns in the X and y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()\n",
    "#print(\"Minority class type 2 in the entire dataset(percentage wise): %0.2f\" % (Counter(y)[2]/len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Applying Hold-Out Testing on Models### \n",
    "-Loading the Hold-out testing strategy from the fist assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.5,random_state=42)\n",
    "print('Actual Class type [1 and 2] feaures in test set: ',Counter(y_test))\n",
    "print('Minority Class Type [2] in test set: ',Counter(y_test)[2])\n",
    "test_neg = Counter(y_test)[2]\n",
    "Minority_test= test_neg/len(y_test)\n",
    "print(\"Minority class in test set percentage wise : %0.2f\" % (Minority_test))\n",
    "print('*' * 20)\n",
    "\n",
    "\n",
    "MLalgos ={}\n",
    "\n",
    "MLalgos['KNN'] = KNeighborsClassifier(n_neighbors=3)\n",
    "MLalgos['DecisionTree']= DecisionTreeClassifier(criterion='entropy')\n",
    "MLalgos['LogRegression'] = LogisticRegression(random_state=42,max_iter=10000)\n",
    "MLalgos['GradBoosting']= GradientBoostingClassifier(random_state=42)\n",
    "bias_calculated ={}\n",
    "accuracy_calculated={}\n",
    "\n",
    "\n",
    "for algo in MLalgos:\n",
    "    print(type(MLalgos[algo]).__name__)\n",
    "    y_predicted = MLalgos[algo].fit(X_train, y_train).predict(X_test)\n",
    "    confusion = confusion_matrix(y_test, y_predicted)\n",
    "    print(\"Confusion matrix is :\\n{}\".format(confusion)) \n",
    "    acc = accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy:  %0.2f' % acc)\n",
    "    accuracy_calculated[algo]=acc\n",
    "    count_predicted = (y_predicted.sum()-len(y_predicted))\n",
    "    bias_calculated[algo]= count_predicted\n",
    "    print(\"Predicted minority class type 2 :\",count_predicted)\n",
    "    pred_neg = Counter(y_predicted)[2]\n",
    "    test_neg = Counter(y_test)[2]\n",
    "    print(\"Predicted minority class type 2 percentage wise : %0.2f\" % (pred_neg/len(y_predicted)))\n",
    "    predicted_count= pred_neg/len(y_predicted)\n",
    "    print('*' * 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Using Synthetic Minority Over-sampling Technique and utilizing it to upsample the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = pd.read_csv('survival.csv')\n",
    "surv['Survived'] = 'GE5'\n",
    "surv.loc[surv['Class']==2,'Survived']='L5'\n",
    "vc=surv['Survived'].value_counts() \n",
    "y = surv.pop('Survived').values\n",
    "surv.pop('Class')\n",
    "X = surv.values\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.5, random_state=42)\n",
    "print(\"Before upsampling training set {}\".format(Counter(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=20, sampling_strategy = 0.7)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "print(\"After sampling training set {}\".format(Counter(y_train_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_bal = {}\n",
    "predictedMinority={}\n",
    "\n",
    "print(\"Total count of Minority class L5 in test set: {}\".format(len(y_test)-Counter(y_test)['GE5']))\n",
    "print('*' * 20)\n",
    "\n",
    "\n",
    "for algo in MLalgos:\n",
    "    print(type(MLalgos[algo]).__name__)\n",
    "    y_pred = MLalgos[algo].fit(X_train_res, y_train_res).predict(X_test)    \n",
    "    predictedMinority[algo] = len(y_pred)-(Counter(y_pred)['GE5'])\n",
    "    print(\"Predicted minority:\", predictedMinority[algo])\n",
    "    acc_bal[algo] = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:  %0.2f' % acc_bal[algo])\n",
    "    print('*' * 20)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using a Pipeline and evaluate the strategy using Cross-Validation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from collections import Counter\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNNPipeline=Pipeline(steps=[('KNN_Classifier',KNeighborsClassifier(n_neighbors=3))])\n",
    "DTPipeline=Pipeline(steps=[('DT_Classifier',DecisionTreeClassifier(criterion='entropy'))])\n",
    "LRPipeline=Pipeline(steps=[('LR_Classifier',LogisticRegression(random_state=42,max_iter=10000))])\n",
    "GBPipeline=Pipeline(steps=[('GB_Classifier',GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "pipelines=[kNNPipeline,DTPipeline,LRPipeline,GBPipeline]  #pipeline will consist of the above 4 models namely KNN, Decision Treee, Logistic Regression,Gradient Boosting\n",
    "pipe_dict = {0: 'KNN', 1: 'Decision Tree', 2: 'Logistic Regression',3:'Gradient Boosting'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_counter=0\n",
    "for pipe in pipelines:\n",
    "    print(\"Classifier:\",pipe_dict[classifier_counter])\n",
    "    accuracy=list()\n",
    "    kf = KFold(n_splits=12)\n",
    "    minority=0\n",
    "    minorityPredicted=0\n",
    "    \n",
    "    for fold, (tr_pointer, ts_pointer) in enumerate(kf.split(X), 1):\n",
    "        X_train = X[tr_pointer]\n",
    "        y_train = y[tr_pointer]\n",
    "        X_test = X[ts_pointer]\n",
    "        y_test = y[ts_pointer]  \n",
    "        X_train_UP, y_train_UP = SMOTE(random_state=20, sampling_strategy = 0.7).fit_resample(X_train, y_train)\n",
    "        pipe.fit(X_train_UP, y_train_UP)  \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        accuracy.append(pipe.score(X_test,y_test))\n",
    "        minority=len(y_test)-Counter(y_test)['GE5'] + minority\n",
    "        minorityPredicted=len(y_pred)-(Counter(y_pred)['GE5']) + minorityPredicted\n",
    "    print(\"Mean Accuracy in 12 folds is: %0.2f\" %(sum(accuracy)/len(accuracy)))\n",
    "    print(\"Total Minority in test folds: \",minority)\n",
    "    print(\"Total Minority predicted in test folds:\",minorityPredicted)\n",
    "    classifier_counter=classifier_counter+1\n",
    "    print('*' * 20)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Outcomes Evaluation of both the methods.\n",
    "\n",
    "- Cross Validation uses K-Folds and is incured with a great computational cost because all of the data is used during the phase of training. It divides the training data into K Different folds. Following it trains the model K times.In the end, the performance is calculated as the average of the K training sets.\n",
    "-  In Hold-out testing, we simply denote the training and testing split. The more the training set, the better will be the the accuracy on the unseen data set.\n",
    "- The hold-out testing has to run only once, hence we can say that It is always faster in computation as compared to the Cross-Valdation\n",
    "- Cross Validation techniques incurs less variation since all of the data is used in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2\n",
    "  ### Objective: To assess the impact of feature selection on Training and Test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTrain=pd.read_csv('heart-train.csv')   #Loading the Heart training data as dataFrame\n",
    "HTest=pd.read_csv('heart-test.csv')     #Loading the Heart test data as dataFrame \n",
    "HTrain.head(10)                         #showing the top 10 rows of the training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif,SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "y_Train=HTrain.pop('DEATH_EVENT').values\n",
    "X_Train=HTrain.values\n",
    "\n",
    "y_Test=HTest.pop('DEATH_EVENT').values\n",
    "X_Test=HTest.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Utilize Gradient Boosting for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier=GradientBoostingClassifier(random_state=42, learning_rate=0.1) #Gradient Boosting Classifier declaration with learning rate as 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Determining Accuracy on the training and test data using all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier.fit(X_Train,y_Train)   #Gradient boosting Classifer trained on Heart_Train data set\n",
    "y_pred=gb_classifier.predict(X_Test) #Gradient Boosting Classifier Tested on the Heart_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Accuracy comes out to be:\",accuracy_score(y_Test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing Information Gain Theory for the feature selection to determine the relevance of attribute in the Heart Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif,SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "iGainScore=mutual_info_classif(X_Train,y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature=pd.DataFrame(iGainScore,index=HTrain.columns,columns=['Information_Gain'])\n",
    "Feature.sort_values(by=['Information_Gain'],ascending=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The features are:\")\n",
    "print(Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical informtion from the feature set\n",
    "- Out of the 12 attributes, Time has the max information Gain. Rest other attributes constitute a very litte(closeness to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_scores = []\n",
    "for count in range(1, X_Train.shape[1]+1):\n",
    "    Feature_transform = SelectKBest(mutual_info_classif, k=count).fit(X_Train,y_Train)\n",
    "    X_new_trainset = Feature_transform.transform(X_Train)\n",
    "    X_new_testset = Feature_transform.transform(X_Test)\n",
    "    seg_NB = gb_classifier.fit(X_new_trainset, y_Train)\n",
    "    y_dash = seg_NB.predict(X_new_testset)\n",
    "    acc = accuracy_score(y_Test, y_dash)\n",
    "    acc_scores.append(acc)\n",
    "\n",
    "Feature['Accuracy'] = acc_scores\n",
    "print(Feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Utilizing sequential feature selection to automatically select subset of the feature that are more relevant to the given problem statement\n",
    "\n",
    "- Our main aim is to reduce the computational cost invloved as well as remove the unnecessary features. This will be helpful in removing errors.\n",
    "- Sequential feauture selection is a wrapper approach that performs addition or removal of attributes based on the performance of the classifier. This process happens until we reach to our desired number of features.\n",
    "- INPUT: the input will be all the attributes of the dataset Provided\n",
    "- OUTPUT: We will have to provide the number of feautures in advance, then the output will be subset of the features provided.In our computation, I'll be providing K_features=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS #Importing sequential feature selector from mlExtend Library\n",
    "feature_names=HTrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_fwd_search = SFS(gb_classifier,      #declared sequential feature search operation with cross-VaLidation fold as 10\n",
    "                  k_features=8, \n",
    "                  forward=True,\n",
    "                  floating=False, \n",
    "                  verbose=1,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_fwd_search.fit(X_Train, y_Train, custom_feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sfs_fwd_search.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "fig1 = plot_sfs(sfs_fwd_search.get_metric_dict(), \n",
    "                ylabel='Accuracy',\n",
    "                kind='std_dev')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.title('Sequential Forward Selection with 8 feautres provided Apriori')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above plot, It can be noticed that the accuracy is highest for the featuresubset=[4,5,6], after that the accuracy decreased for featuresubset=[7,8]  \n",
    "\n",
    "###   Selecting the best feature subset as 5, and then I will be running the Sequential forward search Algorithm with 5  feature sub-set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_fwd_search_subset5 = SFS(gb_classifier, \n",
    "                        k_features=5, \n",
    "                        forward=True, \n",
    "                        floating=False,     \n",
    "                        verbose=1,\n",
    "                        scoring='accuracy',\n",
    "                        cv=10, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs_fwd_search_subset5.fit(X_Train, y_Train, custom_feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset_5=sfs_fwd_search_subset5.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 5 feature set based on the Sequential forward search are:\")\n",
    "print(features_subset_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing sequential backward elimination approach to find out the 5 feature subset by providing forward metric as False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_backward_elimination = SFS(gb_classifier, \n",
    "                  k_features=5, \n",
    "                  forward=False, \n",
    "                  floating=False, \n",
    "                  verbose=1,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_backward_elimination.fit(X_Train, y_Train, \n",
    "                              custom_feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_subset_back_5=seq_backward_elimination.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 5 feature-set based on the Sequential Backward elimination are:\")\n",
    "print(features_subset_back_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding out the feautures that are common to both approaches i.e, Forward search and Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intersection of both feature sets :\",set(features_subset_5).intersection(set(features_subset_back_5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next task: To check the accuracy of the classifier with the subset of feautres identified as significant. They are 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_5=HTrain[['ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier.fit(Xtrain_5,y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_5=HTest[['ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']].values\n",
    "y_predicted=gb_classifier.predict(Xtest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for the Backward Elimination Process\")\n",
    "accuracy_score(y_Test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainFwd_5=HTrain[['anaemia', 'creatinine_phosphokinase', 'platelets', 'smoking', 'time']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier.fit(XtrainFwd_5,y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtestFwd_5=HTest[['anaemia', 'creatinine_phosphokinase', 'platelets', 'smoking', 'time']].values\n",
    "y_predictedFwd=gb_classifier.predict(XtestFwd_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for the Forward Elimination Process\")\n",
    "accuracy_score(y_Test,y_predictedFwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes of the methods: Forward Sequential Search and Backward Elimination\n",
    "- Accuracy obtained from Forward search approach comes out to be 79%\n",
    "- Accuracy obtained from Backward elimination approach comes out to be 85%. Hence, we will be going forward with this method.\n",
    "- Sequenctial forward search and backward elimination have been performed on Heart training dataset and evaluated its efficiency on Heart testing data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating other metrics for evaluating the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = f1_score(y_Test,y_predicted)\n",
    "print('The F1_Score Value is: %0.2f '%f1_score)\n",
    "\n",
    "precision = precision_score(y_Test,y_predicted)\n",
    "print(\"The precision Value is: %0.2f \" %precision)\n",
    "\n",
    "recall = recall_score(y_Test,y_predicted)\n",
    "print(\"The recall Value is : %0.2f \" %recall)\n",
    "\n",
    "acc=accuracy_score(y_Test,y_predicted)\n",
    "print(\"The Accuracy Value is : %0.2f \" %acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes from the performance metrics => F1Score, Precision Value, Recall and Accuracy Score\n",
    "- The Accuracy and the precision value of the classification model is decent enough, with 85 and 80 percent.\n",
    "- The Recall value is low in comparison to other metrics. But since this classificaiton model is of medical diagnosis, Hence the true positive rate is significantly important. It is the value associated with \"the total no. of data predicted postive correctly to the total no. of data that are in class positve.\" It is helpful in the scenarios where our system is predicting the class label as death event and it is actually a death event.\n",
    "- For increasing the performance of the recall, we are considering SMOTE technique to Upsample the training Hearth dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE(random_state=20)\n",
    "X_TrainUP,y_TrainUP=sm.fit_resample(Xtrain_5,y_Train)\n",
    "gb_classifier.fit(X_TrainUP,y_TrainUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sm=gb_classifier.predict(Xtest_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score_sm=accuracy_score(y_Test,y_pred_sm)\n",
    "print(\"The Accuracy score is : %0.2f \" %acc_score_sm)\n",
    "recall_score_sm= recall_score(y_Test,y_pred_sm)\n",
    "print(\"The recall Value is : %0.2f \" %recall_score_sm) \n",
    "precision_sm = precision_score(y_Test,y_pred_sm)\n",
    "print(\"The precision Value is: %0.2f \" %precision_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Outcome\n",
    "- After applying the SMOTE Technique on the Hearth Training data we have identified that The Effects on Evaluation metrics are:\n",
    "- The Accuracy and Recall scores have significant postive effects. The Accuracy rose from 85 to 87% and Recall value rose from 72 to 81\n",
    "- The precision value has little or no effect on Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
